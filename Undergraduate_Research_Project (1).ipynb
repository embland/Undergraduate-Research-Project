{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Undergraduate Research Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxr4AxMFoe4Z"
      },
      "source": [
        "# Indiviudal Research Project:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TnhXjnOomzX"
      },
      "source": [
        "# Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj_CBvbLozx6"
      },
      "source": [
        "# Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import re\n",
        "run_id = 'AllTogether'\n",
        "dir = os.path.join('/content/drive/My Drive/Data/logs', run_id)\n",
        "writer = SummaryWriter(log_dir = dir ) # Initializes TensorBoard\n",
        "# Identify run\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '{dir}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLzhyHEPo-m2"
      },
      "source": [
        "# Custom Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuDaoJcdpSrG"
      },
      "source": [
        "# Together atoi and natural_keys allow 'human sorting'\n",
        "# sorted function resulted in the points and images not aligning cause the frames were sorted by computer logic\n",
        "def atoi(text):\n",
        "    return int(text) if text.isdigit() else text\n",
        "    \n",
        "def natural_keys(text):\n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFdwdPfhp2Tk"
      },
      "source": [
        "# Custom Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKnAR6x7p5xL"
      },
      "source": [
        "# Custom Dataset\n",
        "# img_locs = 'ImagesAnnotated'\n",
        "# pt_locs = 'Annotated Frames'\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_locs, pt_locs, transform = None):\n",
        "        '''\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        img_locs : string\n",
        "            Name of the folder containing the images.\n",
        "        pt_locs : string\n",
        "            Name of the folder containing the points.\n",
        "        transform : function, optional\n",
        "            Put transformation functions you want applied to the data set here\n",
        "\n",
        "        -------\n",
        "        \n",
        "\n",
        "        '''\n",
        "        self.img_locs = img_locs\n",
        "        self.pt_locs = pt_locs\n",
        "        # Get list of all images in folder given as input\n",
        "        self.imglist = list(sorted(os.listdir(os.path.join(os.getcwd(), 'drive', 'MyDrive', 'Data', img_locs))))\n",
        "        self.imglist.sort(key = natural_keys)\n",
        "        # Get list of all points, want points as a tuple integer, not string tho\n",
        "        # To do this need list of all files we want points from\n",
        "        ptfileslist = list(sorted(os.listdir(os.path.join(os.getcwd(), 'drive', 'MyDrive', 'Data', pt_locs))))\n",
        "        pts = [] # Initialize list where we'll store the coord points\n",
        "        fractpts = [] # Initialize list where we'll store the coord points as fractions of the image\n",
        "        for i in range(0, len(ptfileslist)): # runs through all .txt files we want\n",
        "            txtfile = open(os.path.join(os.getcwd(), 'drive', 'MyDrive', 'Data', pt_locs, ptfileslist[i]), 'r') # Open file\n",
        "            for line in txtfile: # runs through .txt file line by line\n",
        "                hcoord = line[18:21] # Pulls out height coord\n",
        "                wcoord = line[23:26] # Pulls out width coord\n",
        "                pts.append((int(hcoord), int(wcoord))) # adds coords to list as ints\n",
        "                fractpts.append(((int(hcoord)/480), (int(wcoord)/640))) # gets points as a fraction of the image height and width\n",
        "            txtfile.close() # closes txtfile\n",
        "        self.points = np.array(pts)  # saves coord list as a callable np.array   \n",
        "        # want points to be fractions of image length and height\n",
        "        self.fract_points = np.array(fractpts)\n",
        "        self.transform = transform\n",
        "        \n",
        "    # Want to be able to call image and points, i.e. need to return a sample \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : the sample you are calling\n",
        "        \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Sample: a dictionary containing an image and coords of a tip \n",
        "\n",
        "        \"\"\"\n",
        "        # Get idxth image\n",
        "        image_name = self.imglist[idx]\n",
        "        image = np.array(Image.open(os.path.join(os.getcwd(), 'drive', 'MyDrive', 'Data', self.img_locs, image_name)))\n",
        "        image = np.moveaxis(image, 2, 0) # moves np array so structure is (channels, h, w) like network will require\n",
        "        # Copy of image (not linked)\n",
        "        cimage = list(image)\n",
        "        cimage = np.array(image)\n",
        "\n",
        "        # Get idxth point\n",
        "        point = list(self.points[idx])\n",
        "        point = np.array(point)\n",
        "        # Copy of point (not linked)\n",
        "        cpoint = list(point)\n",
        "\n",
        "        # Get idxth point as fraction of the image height/width\n",
        "        fractpoint = self.fract_points[idx]\n",
        " \n",
        "        \n",
        "        # Create sample\n",
        "        sample = {'image': image, \n",
        "                  'point': point,\n",
        "                  'fractional point': fractpoint,\n",
        "                  'og point': cpoint }\n",
        "        \n",
        "        # Transform Sample\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        \n",
        "        return sample\n",
        "        \n",
        "    def __len__(self):\n",
        "        # Gets number of samples inside the dataset \n",
        "        return len(self.imglist)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu9ZVJeisqBQ"
      },
      "source": [
        "# Transform Function for Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB4wh9otsv-H"
      },
      "source": [
        "# Transforms \n",
        "class Normalise(object):\n",
        "    \"\"\"\n",
        "    Normalise the intensities in an image\n",
        "    \"\"\"\n",
        "    def __init__(self, min_ = 0, max_ = 1):\n",
        "        self.min = min_\n",
        "        self.max = max_\n",
        "        \n",
        "    def __call__(self, sample):\n",
        "        image, point, fractpoint, og = sample['image'], sample['point'], sample['fractional point'], sample['og point']\n",
        "        # Normalize between self.min and self.max\n",
        "        image = (self.max - self.min) * \\\n",
        "            (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-10) + self.min\n",
        "        # Note normalize comes before ToTensor as assumes image still in np array\n",
        "        # ToTensor should always go last in transform composite \n",
        "        \n",
        "        # Resave to sample\n",
        "        print('HERE')\n",
        "        sample = {'image': image,\n",
        "                  'point': point,\n",
        "                  'fractional point': fractpoint,\n",
        "                  'og point': og }\n",
        "        \n",
        "        return sample\n",
        "\n",
        "\n",
        "class Jitter(object):\n",
        "  \"\"\"\n",
        "  Converts np array to PIL image applies Jitter and converts back to numpy\n",
        "  \"\"\"\n",
        "  def __call__(self, sample):\n",
        "    image, point, fractpoint, og = sample['image'], sample['point'], sample['fractional point'], sample['og point']\n",
        "    image2 = np.moveaxis(image, 0, 2) # Fixes axis to that of typical np array (needed for PIL type)\n",
        "    PILimage = Image.fromarray(image2) # Makes PIL type (necessary for Jitter)\n",
        "    jitterimg = transforms.ColorJitter(.5,.5,.5,.5)(PILimage) # Transforms\n",
        "    npimg = np.array(jitterimg) # Turns jittered img back to np\n",
        "    npimg = np.moveaxis(npimg, 2, 0) # Changes axis back to what program will require later\n",
        "\n",
        "    # Resave to sample\n",
        "    sample = {'image': npimg,\n",
        "              'point': point,\n",
        "              'fractional point': fractpoint,\n",
        "              'og point': og }\n",
        "        \n",
        "    return sample\n",
        "\n",
        "\n",
        "class Flip(object):\n",
        "  \"\"\"\n",
        "  Flips img (left/right or up/down), and adjusts point\n",
        "  \"\"\"\n",
        "  def __call__(self, sample):\n",
        "    image, point, fractpoint, og = sample['image'], sample['point'], sample['fractional point'], sample['og point']\n",
        "    randomint = random.randint(0,1)\n",
        "    if randomint > .5: # randomly decides what kind of flip we use\n",
        "      flipimg = np.flip(image, axis = 1) # Vertial Flip\n",
        "      oglist = list(og)\n",
        "      if np.all(point == oglist):\n",
        "        point[0] = 480 - point[0] # Fixes point to new location\n",
        "        fractpoint[0] = point[0]/480 # Fixes fractional point to new location\n",
        "    else:\n",
        "      flipimg = image \n",
        "    \n",
        "    # Resave to sample\n",
        "    sample = {'image': flipimg,\n",
        "              'point': point,\n",
        "              'fractional point': fractpoint,\n",
        "              'og point': og }\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"\n",
        "    Converts np arrays in Sample to Tensors\n",
        "    \"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, point, fractpoint, og = sample['image'], sample['point'], sample['fractional point'], sample['og point'] # saves img and pt are vars\n",
        "        # Convert to torch\n",
        "        torchimage = torch.from_numpy(image)\n",
        "        torchpoint = torch.from_numpy(point)\n",
        "        torchfractpoint = torch.from_numpy(fractpoint)\n",
        "        # Resave to sample\n",
        "        sample = {'image': torchimage.float(),\n",
        "                  'point': torchpoint.float(),\n",
        "                  'fractional point': torchfractpoint.float(),\n",
        "                  'og point': og }\n",
        "        \n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_sGjkVgtFjf"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXSLNusvuRAF"
      },
      "source": [
        "# Define the neural network\n",
        "model = torchvision.models.alexnet(pretrained=False, progress=True, num_classes = 2, )\n",
        "# Literally just load the alexnet model\n",
        "# We're trying le shit out\n",
        "modelinfo = model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36oLKW2V1S1T"
      },
      "source": [
        "print(modelinfo)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUZ-nq5bGgpB"
      },
      "source": [
        "print(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJOHJT8X3bf3"
      },
      "source": [
        "# Creating Our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H23qsEz3szV"
      },
      "source": [
        "# Create dataset, use ToTensor() to put into Tensor type\n",
        "dataset = Dataset('Images Annotated', 'Annotated Frames', transform = transforms.Compose([Flip(), Jitter(), Normalise(), ToTensor()]) ) # add augmentations to dataset\n",
        "# Split dataset into training and testing 80-20\n",
        "train_ds, test_ds = torch.utils.data.random_split(dataset, (467, 116))\n",
        "# Use Pytorch dataloader \n",
        "train = torch.utils.data.DataLoader(train_ds, batch_size = 10, shuffle = True)\n",
        "test = torch.utils.data.DataLoader(test_ds, batch_size= 1 , shuffle = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0cjA6pxf6gtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "_1Y_OUXK6Xqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqmdzTAd4wob"
      },
      "source": [
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKoR-OoOa4tk"
      },
      "source": [
        "Test DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spyzhuwba1J7"
      },
      "source": [
        "# Test DataLoader in seperate cell so we don't have to always run it\n",
        "for i, batch in enumerate(train):\n",
        "  image_train, point_train = batch['image'], batch['point']\n",
        "  print('Image Train: ', image_train.shape)\n",
        "  print('Point Train: ', point_train.shape)\n",
        "  # Plot the images\n",
        "  image_train = np.array(image_train)\n",
        "  print(image_train.shape)\n",
        "  image_train = np.moveaxis(image_train, 1, 3)\n",
        "  print(image_train.shape)\n",
        "  plt.imshow(image_train[0,:,:,:])\n",
        "  plt.scatter(point_train[0,1], point_train[0,0], c = 'r', s = 10) # plots robot tip point\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  if i > 5:\n",
        "    break\n",
        "\n",
        "len(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpTkrZU020cb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-tFWukDbFt3"
      },
      "source": [
        "# Training the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nN827jhIbLKr"
      },
      "source": [
        "# Training Parameters/Functions\n",
        "epochs = 50 # number of iterations\n",
        "learning_rate = .0001 # learning rate .0001\n",
        "\n",
        "loss_function = torch.nn.MSELoss() # Mean Square Error built-in function \n",
        "optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "loss_list = []\n",
        "valloss_list = []\n",
        "count = 0\n",
        "counter = 0\n",
        "# Training\n",
        "# runs through number of iterations\n",
        "for iterate in range(1, epochs):\n",
        "    # runs through all samples in dataloader train\n",
        "    for i, sample_ in enumerate(train):\n",
        "        \n",
        "        # Clear Gradients\n",
        "        optim.zero_grad()\n",
        "        counter = counter + 1\n",
        "        \n",
        "        # Declare image input (must be in 2nd loop as changes as we go through train dataloader)\n",
        "        img_input = Variable(sample_['image'])\n",
        "        # Declare known point of these images (ground truth)\n",
        "        pt_gt = Variable(sample_['point'])\n",
        "        fractpt_gt = Variable(sample_['fractional point'])\n",
        "        \n",
        "        # Forward Pass\n",
        "        prediction = model(img_input)\n",
        "\n",
        "        # Turns prediction to coord point for loss\n",
        "        x = torch.reshape(prediction[:,0]*480, (-1,1))\n",
        "        y = torch.reshape(prediction[:,1]*640, (-1,1))\n",
        "        # Reshapes predicition into correct format\n",
        "        wholepred = torch.cat((x,y), dim = 1)\n",
        "\n",
        "        # Calculate Loss\n",
        "        current_loss = loss_function(wholepred, pt_gt)\n",
        "        # Add Training Loss to TensorBoard\n",
        "        writer.add_scalars('Loss/Train', {'lr.0001': current_loss}, counter)\n",
        "        # Want counter not epoch, as loss changes for every i running through epoch as well\n",
        "        # Ask Christos and Claudio if should update loss only once for each epoch\n",
        "        \n",
        "        # Store Loss\n",
        "        loss_list.append(current_loss)\n",
        "        \n",
        "        # Backward Pass\n",
        "        current_loss.backward()\n",
        "        \n",
        "        # Update weights\n",
        "        optim.step()\n",
        "        \n",
        "        print('Sample ' + str(i))\n",
        "\n",
        "        \n",
        "        if i == 2:\n",
        "            count = count + 1\n",
        "            fig = plt.figure()\n",
        "            plt.imshow(img_input[0,0,:,:])\n",
        "            plt.scatter(pt_gt[0,1], pt_gt[0,0], c = 'r')\n",
        "            plt.scatter((prediction[0,1].detach().numpy())*640, (prediction[0,0].detach().numpy())*480, c = 'b')\n",
        "            plt.title('Iteration ' + str(count))\n",
        "            writer.add_figure(\"Training/Image.1\", fig, global_step=count)\n",
        "            plt.show()\n",
        "\n",
        "            # Validation \n",
        "            for i, valid in enumerate(test):\n",
        "              with torch.no_grad():\n",
        "                figval = plt.figure()\n",
        "                validimg = valid['image']\n",
        "                validimg = Variable(validimg)\n",
        "                validpts = Variable(valid['point'])\n",
        "                validfracpts = Variable(valid['fractional point'])\n",
        "                valpred = model(validimg)\n",
        "                x2 = torch.reshape(valpred[:,0]*480, (-1,1))\n",
        "                y2 = torch.reshape(valpred[:,1]*640, (-1,1))\n",
        "                wholepred2 = torch.cat((x2,y2), dim = 1)\n",
        "                valloss = loss_function(wholepred2, validpts)\n",
        "                writer.add_scalars('Loss/Valid', { 'lr.0001': valloss}, count)\n",
        "                valloss_list.append(valloss)\n",
        "                plt.imshow(validimg[0,0,:,:])\n",
        "                plt.scatter(validpts[0,1], validpts[0,0], c = 'r')\n",
        "                plt.scatter((valpred[0,1].detach().numpy())*640, (valpred[0,0].detach().numpy())*480, c = 'b')\n",
        "                writer.add_figure(\"Validation/Image.1\", figval, global_step = count)\n",
        "                plt.show()\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    # Prints what epoch we're on     \n",
        "    print('Iteration ' + str(iterate) + ' Completed!')\n",
        "    \n",
        "\n",
        "# flush() method makes sure that all pending events have been written to disk \n",
        "writer.flush()\n",
        "writer.close() # For now we end the writer here but later will want this after we've finished all Writer things (after Testing)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGWgZRjEc9CB"
      },
      "source": [
        "# Testing the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxlddgzPdAXN"
      },
      "source": [
        "for i, sample_test in enumerate(test):\n",
        "    \n",
        "    # Image Data\n",
        "    image_test, point_test, fract_test = sample_test['image'], sample_test['point'], sample_test['fractional point']\n",
        "    image_test_input = Variable(image_test)\n",
        "    \n",
        "    # Prediction\n",
        "    with torch.no_grad(): # No grad involved with test\n",
        "      prediction_test = model(image_test_input)\n",
        "      figtest = plt.figure()\n",
        "      plt.imshow(image_test_input[0, 0, :, :])\n",
        "      plt.scatter(point_test[0,1], point_test[0,0], c = 'r')\n",
        "      plt.scatter((prediction_test[0,1].detach().numpy())*640, (prediction_test[0,0].detach().numpy())*480, c = 'b')\n",
        "      plt.title('Testing Image ' + str(i))\n",
        "      writer.add_figure(\"Test/Image\", figtest, global_step = i)\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP34b1A076L5"
      },
      "source": [
        "lossallaug = loss_list[:]\n",
        "vallossallaug = valloss_list[:]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}